{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade gensim\n",
    "# pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76cdf087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.science.org/doi/10.1126/science.ad...</td>\n",
       "      <td>1</td>\n",
       "      <td>Improved charge extraction in inverted perovsk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.nature.com/articles/s41566-019-0398-2</td>\n",
       "      <td>1</td>\n",
       "      <td>Surface passivation of perovskite film for eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nature.com/articles/s41560-020-007...</td>\n",
       "      <td>1</td>\n",
       "      <td>Intact 2D/3D halide junction perovskite solar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.science.org/doi/10.1126/science.ab...</td>\n",
       "      <td>1</td>\n",
       "      <td>Deterministic fabrication of 3D/2D perovskite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.nature.com/articles/s41467-021-236...</td>\n",
       "      <td>1</td>\n",
       "      <td>Multication perovskite 2D/3D interfaces form v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  label  \\\n",
       "0  https://www.science.org/doi/10.1126/science.ad...      1   \n",
       "1  https://www.nature.com/articles/s41566-019-0398-2      1   \n",
       "2  https://www.nature.com/articles/s41560-020-007...      1   \n",
       "3  https://www.science.org/doi/10.1126/science.ab...      1   \n",
       "4  https://www.nature.com/articles/s41467-021-236...      1   \n",
       "\n",
       "                                                text  \n",
       "0  Improved charge extraction in inverted perovsk...  \n",
       "1  Surface passivation of perovskite film for eff...  \n",
       "2  Intact 2D/3D halide junction perovskite solar ...  \n",
       "3  Deterministic fabrication of 3D/2D perovskite ...  \n",
       "4  Multication perovskite 2D/3D interfaces form v...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# file path\n",
    "relative_path_to_file = os.path.join(\"..\", \"..\", \"data\", \"merged_label.csv\")\n",
    "absolute_path_to_file = os.path.realpath(relative_path_to_file)\n",
    "\n",
    "# read csv file\n",
    "data = pd.read_csv(absolute_path_to_file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8ba346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to keep record of performance metric\n",
    "recall_before = []\n",
    "accuracy_before = []\n",
    "ber_before = []\n",
    "\n",
    "recall_after = []\n",
    "accuracy_after = []\n",
    "ber_after = []\n",
    "\n",
    "# Function to calculate Balanced Error Rate\n",
    "def balanced_error_rate(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    ber = 1 - (sensitivity + specificity) / 2\n",
    "    return ber\n",
    "\n",
    "# Model Evaluation Function with Train/Test Accuracy\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\"Model\"):\n",
    "    # Predictions on test and train sets\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    # Classification report for recall\n",
    "    report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "    test_recall = report['1']['recall'] \n",
    "    \n",
    "    # Balanced Error Rate for test set\n",
    "    test_ber = balanced_error_rate(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"\\nEvaluation Report for {model_name}:\\n\")\n",
    "    print(\"Classification Report (Test Set):\\n\", classification_report(y_test, y_pred_test))\n",
    "    print(\"Confusion Matrix (Test Set):\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Test Recall): {test_recall}\")\n",
    "    print(\"Balanced Error Rate (Test Set):\", test_ber)\n",
    "    \n",
    "    # Return metrics as dictionary for further use\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_recall\": test_recall,\n",
    "        \"test_ber\": test_ber\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf37920",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Step 1: Train Word2Vec model on your corpus\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]]  \u001b[38;5;66;03m# Tokenize text\u001b[39;00m\n\u001b[1;32m      8\u001b[0m word2vec_model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Transform each document into an averaged Word2Vec vector\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Step 1: Train Word2Vec model on your corpus\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]]  \u001b[38;5;66;03m# Tokenize text\u001b[39;00m\n\u001b[1;32m      8\u001b[0m word2vec_model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Transform each document into an averaged Word2Vec vector\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Train Word2Vec model on your corpus\n",
    "sentences = [text.split() for text in data['text']]  # Tokenize text\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Step 2: Transform each document into an averaged Word2Vec vector\n",
    "def document_vector(doc):\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.index_to_key]\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0) if len(doc) > 0 else np.zeros(100)\n",
    "\n",
    "X = np.array([document_vector(text.split()) for text in data['text']])\n",
    "y = data['label']\n",
    "\n",
    "# Step 3: Split data and train Random Forest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate model\n",
    "predictions = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95af16c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Logistic Regression\u001b[39;00m\n\u001b[1;32m      2\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m----> 3\u001b[0m lr_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m      4\u001b[0m temp \u001b[38;5;241m=\u001b[39m evaluate_model(lr_model, X_train, X_test, y_train, y_test, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m recall_before \u001b[38;5;241m=\u001b[39m recall_before \u001b[38;5;241m+\u001b[39m [temp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_recall\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(lr_model, X_train, X_test, y_train, y_test, model_name=\"Logistic Regression\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13b2a593",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Naive Bayes\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nb_model \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m----> 3\u001b[0m nb_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m      4\u001b[0m temp \u001b[38;5;241m=\u001b[39m evaluate_model(nb_model, X_train, X_test, y_train, y_test, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaive Bayes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m recall_before \u001b[38;5;241m=\u001b[39m recall_before \u001b[38;5;241m+\u001b[39m [temp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_recall\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(nb_model, X_train, X_test, y_train, y_test, model_name=\"Naive Bayes\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm_model = LinearSVC(random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(svm_model, X_train, X_test, y_train, y_test, model_name=\"SVM\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(rf_model, X_train, X_test, y_train, y_test, model_name=\"Random Forest\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, model_name=\"XGBoost\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7c7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433e2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bc93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc72ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c7b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63071a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Doc2Vec, TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Prepare data for Doc2Vec\n",
    "tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(data['text'])]\n",
    "doc2vec_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=2, epochs=20)\n",
    "\n",
    "# Step 2: Create document vectors for each document in the dataset\n",
    "X = np.array([doc2vec_model.infer_vector(text.split()) for text in data['text']])\n",
    "y = data['label']\n",
    "\n",
    "# Step 3: Split data and train Random Forest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate model\n",
    "predictions = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00fc49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(lr_model, X_train, X_test, y_train, y_test, model_name=\"Logistic Regression\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147508f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(nb_model, X_train, X_test, y_train, y_test, model_name=\"Naive Bayes\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142dfa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm_model = LinearSVC(random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(svm_model, X_train, X_test, y_train, y_test, model_name=\"SVM\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(rf_model, X_train, X_test, y_train, y_test, model_name=\"Random Forest\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, model_name=\"XGBoost\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fce09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945ee0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cac3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load SBERT model and create embeddings for each document\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "X = sbert_model.encode(data['text'].tolist())\n",
    "y = data['label']\n",
    "\n",
    "# Step 2: Split data and train Random Forest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate model\n",
    "predictions = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cea583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(lr_model, X_train, X_test, y_train, y_test, model_name=\"Logistic Regression\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d81b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(nb_model, X_train, X_test, y_train, y_test, model_name=\"Naive Bayes\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm_model = LinearSVC(random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(svm_model, X_train, X_test, y_train, y_test, model_name=\"SVM\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152da3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(rf_model, X_train, X_test, y_train, y_test, model_name=\"Random Forest\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e10669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, model_name=\"XGBoost\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a4414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d41bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c37fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load USE model from TensorFlow Hub\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Step 2: Generate embeddings for each document\n",
    "texts = data['text'].tolist()\n",
    "embeddings = use_model(texts).numpy()  # Convert tensor to numpy array for compatibility\n",
    "\n",
    "# Step 3: Prepare labels\n",
    "y = data['label']\n",
    "\n",
    "# Step 4: Split data and train Random Forest\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "predictions = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47bce048",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Step 2: Tokenize the text data and get embeddings\u001b[39;00m\n\u001b[1;32m     12\u001b[0m texts \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3021\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3020\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3021\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3109\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3104\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3105\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3107\u001b[0m         )\n\u001b[1;32m   3108\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 3109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3111\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   3132\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3133\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3151\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3152\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3302\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3303\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3304\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3308\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3309\u001b[0m )\n\u001b[0;32m-> 3311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3313\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3329\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:529\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 529\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    541\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    543\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    553\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load ELECTRA model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "model = AutoModel.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "# Step 2: Tokenize the text data and get embeddings\n",
    "texts = data['text'].tolist()\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Use mean pooling to get a single embedding per sentence/document\n",
    "embeddings = outputs.last_hidden_state.mean(dim=1).numpy()  # Shape: (num_documents, embedding_dim)\n",
    "\n",
    "# Step 3: Prepare labels\n",
    "y = data['label']\n",
    "\n",
    "# Step 4: Split data and train Random Forest\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "predictions = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f2a8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(rf_model, X_train, X_test, y_train, y_test, model_name=\"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(lr_model, X_train, X_test, y_train, y_test, model_name=\"Logistic Regression\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a516d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "temp = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, model_name=\"XGBoost\")\n",
    "recall_before = recall_before + [temp.get('test_recall')]\n",
    "accuracy_before = accuracy_before + [temp.get('test_accuracy')]\n",
    "ber_before = ber_before + [temp.get('test_ber')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0bedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1be8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395711f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a65b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9882612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c69765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4880d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
